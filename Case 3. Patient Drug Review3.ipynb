{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Case 3: Patient Drug Review"},{"metadata":{},"cell_type":"markdown","source":"**TEAM 10: Eliecer Diaz, Muskan Kaushik, and Zakaria Hasan**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Introduction**\n\nText analysis is one of the everyday machine learning technics that can be solved by deep learning. Deep learning architecture uses a neural network.\nRecurrent neural networks (RNN) are at the forefront of the neural network models used for learning from sequential data. \nThis document aims to investigate sentiment analysis on patient drug reviews. The computation was implemented by using Long Short Term Memory(LSTM) to examine the dataset and construct effective model to predict the rating based on given reviews. \n\n\n**Objectives:**\n* Predict the multiclass type of drug rating based on the drug reviews \n* give metrics about the accuracy, Cohen's Kappa, Classification report\n* 3 categories from the numeric drug review ratings and try to classify converted "},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random # Random generators\nimport numpy as np\nimport pandas as pd # Pandas dataframe\nimport matplotlib.pyplot as plt\nimport re # Text cleaning\nimport nltk # Text processing\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom bs4 import BeautifulSoup # Text cleaning\nimport tensorflow as tf # Tensorflow\nfrom tensorflow.keras import preprocessing # Text preprocessing\nfrom tensorflow.keras.preprocessing.text import Tokenizer # Text preprocessing\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # Text preprocessing\nfrom tensorflow.keras.models import Sequential # modeling neural networks\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Embedding, SpatialDropout1D, LSTM\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import optimizers, metrics # Neural Network\nfrom sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\nrandom.seed(10) # Set seed for the random generators\nprint(f\"Tensorflow version: {tf.__version__}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create dataframes train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/drugsComTrain_raw.csv')\ntest = pd.read_csv('../input/drugsComTest_raw.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the column names and dataset sizes"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.values.shape[0], test.values.shape[0],\ntrain.values.shape[0]/test.values.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train set is almost exactly 3 times as big as test set. This is a typycal 75:25 train:test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train shape :\" ,train.shape)\nprint(\"Test shape :\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rating Distribution "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.rating.hist(color = 'skyblue')\nplt.title('Distribution of Ratings')\nplt.xlabel('Rating')\nplt.xticks([i for i in range(1,11)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This distribution illustrate that people generally write review for drugs they really like or those that they rally dislike. There are fewer middle rating as compared to extreme ratings."},{"metadata":{},"cell_type":"markdown","source":"# Hadling text with Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n# For Train Data\nsamples = train['review']\ntokenizer =Tokenizer(num_words = 5000)\ntokenizer.fit_on_texts(samples)\n# For Test Data\ntest_samples = test['review']\ntest_tokenizer =Tokenizer(num_words = 5000)\ntest_tokenizer.fit_on_texts(test_samples)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert text to sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert text to sequences for Train Data\nsequences = tokenizer.texts_to_sequences(samples)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n\n# Convert text to sequences for Test Data\ntest_sequences = test_tokenizer.texts_to_sequences(test_samples)\ntest_word_index = test_tokenizer.word_index\nprint('Found %s unique tokens.' % len(test_word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make one hot samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences \n\ndata = pad_sequences(sequences, maxlen=200)\ndata.shape\n\ntest_data = pad_sequences(test_sequences, maxlen=200)\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorize labels"},{"metadata":{},"cell_type":"markdown","source":"2 Mean the patient review is POSITIVE <br>\n1 Mean the patient review is NEUTRAL <br>\n0 Mean the patient review is NEGATIVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorize labels for Train Data\nlabels = train ['rating'].values\nlabels = 1.0 * (labels >= 6 ) + 1.0*(labels >= 4)\n\n\n# Categorize labels for Test Data\ntest_labels = test ['rating'].values\ntest_labels = 1.0 * (test_labels >= 6 ) + 1.0*(test_labels >= 4)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One hot code the output values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# For train and validation \nlabels = to_categorical(np.asarray(labels))#\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\n# For Test data\ntest_labels = to_categorical(np.asarray(test_labels))\nprint('Shape of data tensor:', test_data.shape)\nprint('Shape of label tensor:', test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split into traning and validition "},{"metadata":{"trusted":true},"cell_type":"code","source":"VALIDATION_SPLIT = 0.25\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1"},{"metadata":{},"cell_type":"markdown","source":"\n\nIn this model, we use nine layers contain one Embedding layer, three conv1D,one MaxPooling, one GlobalMaxPooling and two Dense layer. \n\nEmbedding layer is for text processing.The layer will turn the number that encoded from words into vectors.\n\nConv1D layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs.\n\nMaxPooling1D creates a operation for temporal data.\n\nGlobalMaxPooling1D for temporal data takes the max vector over the steps dimension.\n\nDense layer represents a matrix vector multiplication. (assuming your batch size is 1) The values in the matrix are the trainable parameters which get updated during backpropagation.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model\nfrom keras import regularizers\n\n\nembedding_layer = Embedding(5000,\n                            100,\n                            input_length=200,\n                            trainable=True)\n\n\nsequence_input = Input(shape=(200,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.05))(x)\npreds = Dense(3, activation='softmax')(x)\n\n\nmodel1 = Model(sequence_input, preds)\nmodel1.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"history = model1.fit(x_train, y_train,\n          batch_size=32,\n          epochs=15,\n          verbose=0,\n          validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot the accuracy and loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ne = arange(len(acc)) + 1\n\nplot(e, acc, label = 'train')\nplot(e, val_acc, label = 'validation')\ntitle('Training and validation accuracy')\nxlabel('Epoch')\ngrid()\nlegend()\n\nfigure()\n\nplot(e, loss, label = 'train')\nplot(e, val_loss, label = 'validation')\ntitle('Training and validation loss')\nxlabel('Epoch')\ngrid()\nlegend()\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate metrics"},{"metadata":{},"cell_type":"markdown","source":"# Find the predicted values for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = argmax(model1.predict(x_val), axis = 1)\ny_true = argmax(y_val, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the classification report for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cr = classification_report(y_true, y_pred)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the confusion matrix for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred).T\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the cohen's kappa, both with linear and quadratic weights for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the cohen's kappa, both with linear and quadratic weights\nk = cohen_kappa_score(y_true, y_pred)\nprint(f\"Cohen's kappa (linear)    = {k:.3f}\")\nk2 = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\nprint(f\"Cohen's kappa (quadratic) = {k2:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the predicted values for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = argmax(model1.predict(test_data), axis = 1)\ny_true = argmax(test_labels, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the classification report for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cr = classification_report(y_true, y_pred)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the confusion matrix for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred).T\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the cohen's kappa, both with linear and quadratic weights for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the cohen's kappa, both with linear and quadratic weights\nk = cohen_kappa_score(y_true, y_pred)\nprint(f\"Cohen's kappa (linear)    = {k:.3f}\")\nk2 = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\nprint(f\"Cohen's kappa (quadratic) = {k2:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#  Model 2"},{"metadata":{},"cell_type":"markdown","source":"In this model, we use five layers contain one Embedding layer, one SpatialDropout1D layer, two LSTM layers and one Dense layer.\n\nEmbedding layer is for text processing.The layer will turn the number that encoded from words into vectors.\n\nSpatialDropout1D layer is like a dropout function. We input rate(0.1) in the function and fraction of the input units will be droped.\n\nLSTM layer is a text prediction layer.The LSTM is improved from simpleRNN network.It helps us solve 'vanishing gradient' problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(5000, 128, input_length=200))\nmodel.add(SpatialDropout1D(0.1))\nmodel.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))\nmodel.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"history2 = model.fit(x_train, y_train,\n          batch_size=32,\n          epochs=15,\n          verbose=0,\n          use_multiprocessing=True,  \n          validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history2.history['acc']\nval_acc = history2.history['val_acc']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\ne = arange(len(acc)) + 1\n\nplot(e, acc, label = 'train')\nplot(e, val_acc, label = 'validation')\ntitle('Training and validation accuracy')\nxlabel('Epoch')\ngrid()\nlegend()\n\nfigure()\n\nplot(e, loss, label = 'train')\nplot(e, val_loss, label = 'validation')\ntitle('Training and validation loss')\nxlabel('Epoch')\ngrid()\nlegend()\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the predicted values for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = argmax(model.predict(x_val), axis = 1)\ny_true = argmax(y_val, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the classification report for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cr = classification_report(y_true, y_pred)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the confusion matrix for the validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred).T\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the cohen's kappa, both with linear and quadratic weights for the Validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"k = cohen_kappa_score(y_true, y_pred)\nprint(f\"Cohen's kappa (linear)    = {k:.3f}\")\nk2 = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\nprint(f\"Cohen's kappa (quadratic) = {k2:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the predicted values for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = argmax(model.predict(test_data), axis = 1)\ny_true = argmax(test_labels, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the classification report for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cr = classification_report(y_true, y_pred)\nprint(cr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the confusion matrix for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred).T\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate the cohen's kappa, both with linear and quadratic weights for the Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the cohen's kappa, both with linear and quadratic weights\nk = cohen_kappa_score(y_true, y_pred)\nprint(f\"Cohen's kappa (linear)    = {k:.3f}\")\nk2 = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\nprint(f\"Cohen's kappa (quadratic) = {k2:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let compare the results with Gr√§sser et al. Table 2, p. 124.The results obtained with the results in the book.The accuracy of the data is not too different. The results from the book have an accuracy of around 90 percent while the results of our models can predict up to 80 percent.But the inter-rater reliability or Cohen's kappa of the models in the books compare with our models are quite distinct.The Cohen's kappa values from the books have almost 84, while the Cohen's kappa values of our models are around from 60 to 70."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In conclusion, in the case 3 what we did are text anylysis for predicting drug review catigories.First we start with importing data, text preprocessing, build model and vesulize the result respectively.By doing text anylysis, it helps us to explore text data and also text prediction as well.we can use text analysis in various field such as  Sentiment analysis,Language recognition,Automatization of customer service,Spam email filtering, and others.Over all It was a great experience for us.\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}